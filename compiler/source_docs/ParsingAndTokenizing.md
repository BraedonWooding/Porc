# Parsing and Tokenizing

This is a relatively critical part of the compiler since the parsing is done in a relatively specific way.

## Tokenizing

Basically we implement a psuedo stream like generator for tokens, the benefit of going a generator route is that we don't pay the memory cost of a large list of tokens (around about 41 bytes per token) but we may a small perf cost of not having the IO being so streamlined (supposedly)

Just to add a bit more context; I'm not actually sure that we even pay the performance cost due to the fact that when this gets parallelised it is better to interweave the IO calls together rather than having them all bunched up at the start (since if you parallelise it but require the IO to occur at the same time you'll just get a huge IO bottleneck), so I actually think this could be more performant.  Of course I would need to test it to prove this so I will eventually get around to that (since it should be a really easy thing to test) but I want to test it on a finished compiler.

We parse tokens through an in-memory trie basically we don't pay the cost of losing locality with pointers (everything is nicely cache oriented) but we pay the cost of having to generalise the memory structure meaning that we take up significantly more memory (currently it uses a couple of kb+).  Also it uses pointers due to a syntax oddity but I'll get that fixed soon too (just gotta do a union with a pointer tbh since if all the array is 0 then it is empty anyway).  I can actually make it even better using constexpr since I should be able to just size the arrays to the minimum required size and basically pay 0 memory cost whatsoever, which will be amazing!

>> Side note: this will bump up the compile time significantly but since the files are autogenerated they should be cached and never regenerated unless we make a change to the files that generate them (which only occurs with new syntax).

Regardless this kind of parsing of tokens is extremely efficient and basically we end up with the following;

- a token for every kind of symbol
  - i.e. `-` and `->` are different symbols and it'll parse them correctly using the tokenizer
- a token for integers, and floating points separately
- a token for characters, and strings
- tokens for all reserved words (and a few common types to make life easy)
- a token for identifiers that aren't reserved (just a catchall)
- and an error for if it can't figure out what the token is
  - This could be improved by trying to figure out what it would want
    - Currently it is kinda clever and returns 'undefined' so it prints something semi sensible but always this could be better (prints 'expecting x token got y').

## Parsing

Basically parsing is single threaded in a way but is built so you can parallelise horizontally (that is spawning a parser for each file) basically in Porc you don't 'include' other files it just presumes that you mean to include all files.

>> This simplifies a lot of the annoyance with python separation, subfolders are accessible through a 'module' like thing automatically (kinda like golang).

You can view the parser as a psuedo stream filter, basically it does something like;

```c
                                  Error Stream
"String", 2.32293, ... | Parser >>
                                  AST
```

Now why do we have an error stream?  Well because interestingly enough Porc makes a smart decision to ONLY go to the next step of parsing (semantic analysis) if the syntactical parsing succeeds, so what will happen is if you make a series of syntax errors you'll get a nice list of syntax errors but no weird type errors due to the symbols not being defined which can cause a lot of confusion.  Furthermore the errors often carry quite a bit of information and often we give pretty smart errors (mainly due to how the syntax was designed).
